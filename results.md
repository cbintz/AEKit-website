---
layout: page
title: Results
---

**Findings**

Interactive Web Demo:
Our analysis involved running nine celebrity photos in Open Face’s model using a database of 60 celebrity photos collected from Labeled Faces in the Wild and Google image searches. We then selected the top 10 closest images for each of the nine celebrity photos to include in our demo. Of all the nine celebrity photos, the maximum distance score of nine top 10 closest images was 1.409, between a photo of Paris Hilton and Joy Bryant, and the minimum distance score was 0.116, between two different photos of LeBron James. Overall, celebrities with lighter skin tones had lower similarity scores than celebrities with darker skin tones.

Our findings of differences in similarity scores along the lines of skin tone are consistent with the literature surrounding facial recognition software and accuracy according to skin tone (Buolamwini and Gebru 2018). Buolamwini and Gebru (2018)’s work demonstrates that increased diversity in the training set can increase a model’s accuracy for intersectional identities, such as  for people and women of color. OpenFace trains its model on 13 public and 1 private dataset. The scope and time limitations of our project prevented us from exploring the breakdown of these datasets in regards to skin tone and gender. Future work could explore this breakdown to determine if lack of diversity in the training datasets may be responsible for higher similarity scores for people of color in our demo analysis.



**Deliverables**

We designed an Algorithmic Equity Toolkit, a set of tools for identifying and auditing algorithmic processes used in the public sector, especially of automated decision-making and predictive technologies. The toolkit has three components:
Surveillance and ADS Identification Guide, for distinguishing surveillance and ADS’s and their different functions. This will help civil rights advocates ask the right questions depending on the technology type.

Questionnaire, for surfacing the social context of a given system, its technical failure modes (i.e., potential for not working correctly, such as false positives), and its social failure modes (i.e. its potential for discrimination when working correctly). The questionnaire is a list of sample questions that users can use to inquire about the potential harms of surveillance or ADS technologies.

An interactive facial recognition false positive web demo, that illustrates the underlying harms and mechanics of facial recognition technology, one of the technologies in the questionnaire. Our interactive demo illustrates false positives, disparities in accuracy along lines of race and gender, and the potential harms with choosing too low a threshold to determine matches, resulting in false positives. Our demo aims to illustrate an algorithmic harm in an accessible way and involves interactivity for an engaging user experience.


The primary users of the Algorithmic Equity Toolkit will be community members, including civil rights advocacy and grassroots organizations and anyone interested in algorithmic equity. We hope community members will be better empowered with this toolkit to hold government agencies accountable for the technologies they implement in their communities. The toolkit can be used when engaging with policymakers, government representatives, or when users want to learn more about surveillance and ADS technologies and their potential harms.

With the toolkit, civil rights activists, grassroots organizers, and community members can identify a government surveillance and an ADS technology, how they work, and what are the potential social justice implications and harms with using the public sector systems. An ADS is a computerized implementation of algorithms to assist in decision-making. ADS’s are increasingly used in our society to analyze data and make decisions more quickly and efficiently; however, the increasing use of ADS’s decreases transparency and accountability due to their complexity and the lack of awareness about how they work.

We hope that with this toolkit civil rights activists can distinguish between surveillance tools from ADS tools and be empowered to challenge the implementation and expansion of both surveillance and ADS technologies by asking the right questions.


**Outcomes**

ACLU, WA as the primary stakeholder has provided connections to other community organizations such as Densho, CAIR who are members of the Tech Fairness Coalition. The organizations have provided insights, feedback and suggestions on the toolkit design and how to make it accessible for non-technical community members. The organizations have expressed interest in using the toolkit and sharing it on their websites and social media accounts.

The primary goal of this project is to empower community members with a toolkit that helps them ask questions about algorithmic technologies and biases to their elected officials. In addition, we hope the toolkit will help inform local and national technology policy changes and lead to algorithmic equity.

The City of Seattle and Washington State are both world leaders in technology policy. The Washington State House has drafted a tech fairness bill (HB 1655) a first step in the direction of broad algorithmic regulation. However, previous research indicates that even expert policymakers are not prepared to understand the particular risks of algorithmic systems as such. We anticipate the toolkit to be adopted both within government and by policy advocates such as the ACLU to strengthen HB 1655 and other existing, ongoing, and future regulatory efforts.
